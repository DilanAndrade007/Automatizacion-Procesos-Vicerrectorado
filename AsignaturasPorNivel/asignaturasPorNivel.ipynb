{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696cc912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlsxwriter in c:\\users\\andra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7e645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado: data\\malla_administracion_empresas.pdf -> outputs/malla_administracion_empresas_codigos_por_nivel.csv\n",
      "Procesado: data\\malla_agroindustria.pdf -> outputs/malla_agroindustria_codigos_por_nivel.csv\n",
      "Procesado: data\\malla_ambiental.pdf -> outputs/malla_ambiental_codigos_por_nivel.csv\n",
      "Procesado: data\\malla_ciencia_datos_IA.pdf -> outputs/malla_ciencia_datos_IA_codigos_por_nivel.csv\n",
      "Procesado: data\\malla_civil.pdf -> outputs/malla_civil_codigos_por_nivel.csv\n",
      "Procesado: data\\malla_computacion.pdf -> outputs/malla_computacion_codigos_por_nivel.csv\n",
      "Procesado: data\\malla_economia.pdf -> outputs/malla_economia_codigos_por_nivel.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import statistics\n",
    "\n",
    "# ===================== PARÁMETROS AJUSTABLES =====================\n",
    "FOLDER_PATH = \"data/\"\n",
    "OUTPUT_FOLDER = \"outputs/\"\n",
    "RIGHT_CUT_FRAC = 0.74          # corte para ignorar columna derecha (x1 <= width * RIGHT_CUT_FRAC)\n",
    "LEVEL_LEFT_MARGIN_FRAC = 0.08  # para detectar los dígitos grandes de nivel a la izquierda\n",
    "HEIGHT_MIN_FRAC = 0.88         # descarta \"words\" cuyo alto < (mediana_alto * este_factor)\n",
    "MIN_HEIGHT_PX = 7.5            # umbral de seguridad: si la mediana falla, usa este mínimo absoluto\n",
    "# ================================================================\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# PDFs a procesar\n",
    "pdf_files = glob.glob(os.path.join(FOLDER_PATH, \"*.pdf\"))\n",
    "\n",
    "# Regex de códigos: 4 letras + 3 dígitos\n",
    "CODE_RE = re.compile(r\"\\b[A-Z]{4}\\d{3}\\b\")\n",
    "\n",
    "# Mapa de símbolos a espacio para normalizar (flechas, guiones, bullets)\n",
    "ARROW_CHARS = [\n",
    "    \"→\",\"⇒\",\"➔\",\"⟶\",\"⟼\",\"⟿\",\"↦\",\"↠\",\"➤\",\"▶\",\"❯\",\"›\",\"»\",\n",
    "    \"-\", \"—\", \"–\", \"·\", \"•\", \"‣\", \"∙\", \"·\"\n",
    "]\n",
    "ARROW_TABLE = str.maketrans({ch: \" \" for ch in ARROW_CHARS})\n",
    "\n",
    "def normalize_token(txt: str) -> str:\n",
    "    # Limpia flechas y símbolos, compacta espacios\n",
    "    txt = (txt or \"\").translate(ARROW_TABLE)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt.strip())\n",
    "    return txt\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    file_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    levels_codes = defaultdict(list)\n",
    "    carrera_name = \"DESCONOCIDA\"\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # ------ Carrera ------\n",
    "        first_text = pdf.pages[0].extract_text() or \"\"\n",
    "        m = re.search(r\"Carrera:\\s*(.+)\", first_text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            carrera_name = m.group(1).strip()\n",
    "\n",
    "        for page in pdf.pages:\n",
    "            width, height = page.width, page.height\n",
    "\n",
    "            # Extraemos palabras con coordenadas\n",
    "            words = page.extract_words(\n",
    "                x_tolerance=2, y_tolerance=2,\n",
    "                keep_blank_chars=False, use_text_flow=True\n",
    "            )\n",
    "\n",
    "            # Ignorar columna derecha\n",
    "            right_cut = width * RIGHT_CUT_FRAC\n",
    "            left_words = [w for w in words if w[\"x1\"] <= right_cut]\n",
    "\n",
    "            # ------------------ FILTRO POR TAMAÑO ------------------\n",
    "            # Calculamos la mediana del alto de los words (proxy de font-size)\n",
    "            heights = [(w[\"bottom\"] - w[\"top\"]) for w in left_words if w[\"bottom\"] > w[\"top\"]]\n",
    "            if heights:\n",
    "                med_h = statistics.median(heights)\n",
    "                height_threshold = max(med_h * HEIGHT_MIN_FRAC, MIN_HEIGHT_PX)\n",
    "            else:\n",
    "                height_threshold = MIN_HEIGHT_PX\n",
    "\n",
    "            # Dejamos solo words \"grandes\" (evita códigos chiquitos de flechas/prerrequisitos)\n",
    "            left_words_big = [w for w in left_words if (w[\"bottom\"] - w[\"top\"]) >= height_threshold]\n",
    "\n",
    "            # ------------------ DETECCIÓN DE NIVELES ------------------\n",
    "            level_markers = [\n",
    "                {\"text\": w[\"text\"], \"y\": (w[\"top\"] + w[\"bottom\"]) / 2}\n",
    "                for w in left_words_big\n",
    "                if w[\"text\"] in list(\"123456789\") and w[\"x0\"] < width * LEVEL_LEFT_MARGIN_FRAC\n",
    "            ]\n",
    "            level_markers.sort(key=lambda d: d[\"y\"])\n",
    "\n",
    "            # Deduplicar por proximidad vertical\n",
    "            dedup_levels = []\n",
    "            for lm in level_markers:\n",
    "                if not dedup_levels or abs(lm[\"y\"] - dedup_levels[-1][\"y\"]) > 8:\n",
    "                    dedup_levels.append(lm)\n",
    "\n",
    "            # Rango Y por nivel\n",
    "            ranges = []\n",
    "            for i, lm in enumerate(dedup_levels):\n",
    "                y_top = lm[\"y\"] - 24\n",
    "                y_bottom = (dedup_levels[i+1][\"y\"] - 24) if (i+1 < len(dedup_levels)) else height\n",
    "                try:\n",
    "                    lvl = int(lm[\"text\"])\n",
    "                    ranges.append((lvl, y_top, y_bottom))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # ------------------ EXTRACCIÓN DE CÓDIGOS ------------------\n",
    "            # Normalizamos texto (para no perder códigos pegados a flechas/símbolos)\n",
    "            for w in left_words_big:\n",
    "                cleaned = normalize_token(w[\"text\"]).upper()\n",
    "                # Buscar TODAS las coincidencias (si vienen pegadas)\n",
    "                for tok in re.findall(r\"[A-Z]{4}\\d{3}\", cleaned):\n",
    "                    if CODE_RE.fullmatch(tok):\n",
    "                        y_center = (w[\"top\"] + w[\"bottom\"]) / 2\n",
    "                        # Asignar por nivel según rango vertical\n",
    "                        for lvl, y0, y1 in ranges:\n",
    "                            if y0 <= y_center < y1:\n",
    "                                levels_codes[lvl].append(tok)\n",
    "                                break\n",
    "\n",
    "    # Limpiar duplicados por nivel manteniendo orden\n",
    "    for lvl in levels_codes:\n",
    "        seen = set()\n",
    "        ordered = []\n",
    "        for c in levels_codes[lvl]:\n",
    "            if c not in seen:\n",
    "                ordered.append(c)\n",
    "                seen.add(c)\n",
    "        levels_codes[lvl] = ordered\n",
    "\n",
    "    # Armar filas\n",
    "    rows = []\n",
    "    for lvl in sorted(levels_codes.keys()):\n",
    "        for code in levels_codes[lvl]:\n",
    "            rows.append({\n",
    "                \"archivo\": file_name,\n",
    "                \"carrera\": carrera_name,\n",
    "                \"nivel\": lvl,\n",
    "                \"codigo\": code\n",
    "            })\n",
    "\n",
    "    # Guardar por archivo\n",
    "    df = pd.DataFrame(rows).sort_values([\"nivel\", \"codigo\"]).reset_index(drop=True)\n",
    "    out_csv = os.path.join(OUTPUT_FOLDER, f\"{file_name}_codigos_por_nivel.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Procesado: {pdf_path} -> {out_csv}\")\n",
    "\n",
    "    all_rows.extend(rows)\n",
    "\n",
    "# Consolidado\n",
    "df_all = pd.DataFrame(all_rows).sort_values([\"archivo\", \"nivel\", \"codigo\"]).reset_index(drop=True)\n",
    "df_all.to_csv(os.path.join(OUTPUT_FOLDER, \"todas_mallas_codigos.csv\"), index=False)\n",
    "print(\"✅ Listo. Consolidado en outputs/todas_mallas_codigos.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo generado con observaciones en: outputs/Reporte_AsignaturasPorNivel.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------- EXCEPCIONES: estos códigos se ignoran al evaluar conflictos --------\n",
    "EXCEPTION_CODES = {\n",
    "    # (anteriores)\n",
    "    \"TITD101\",\"TITD201\",\"MATD113\",\"MATD123\",\"FISD134\",\"MATD213\",\"MATD223\",\"MATD115\",\n",
    "    \"MATD124\",\"MATD133\",\"MATD141\",\"MATD224\",\"MATD234\",\"MATD314\",\"FISD113\",\"MATD143\",\n",
    "    \"MATD153\",\"CSHD111\",\"CSHD211\",\"AMBD261\",\"CSHD162\",\"CSHD262\",\"CSHD362\",\"ADMD511\",\"ADMD611\",\"ADMD711\",\"ADMD163\",\n",
    "    \"ADMD421\",\"ICOD111\",\"ICOD151\",\"ICOD142\",\"ICOD173\",\"ICOD273\",\"DEPD110\",\"DEPD120\",\n",
    "    \"ADMD700\",\"ADMD800\",\"AMBD900\",\"CSHD600\",\"CSHD311\",\"CSHD321\",\"CSHD331\",\"CSHD341\",\n",
    "    \"CSHD351\",\"CSHD361\",\"CSHD371\",\"CSHD381\",\"CSHD391\",\"CSHD3A1\",\"CSHD3B1\",\"CSHD411\",\n",
    "    \"CSHD421\",\"CSHD431\",\"CSHD441\",\"CSHD451\",\"CSHD510\",\"CSHD520\"\n",
    "}\n",
    "\n",
    "def is_exception(code: str) -> bool:\n",
    "    \"\"\"\n",
    "    Regla centralizada de excepciones:\n",
    "    - Códigos listados en EXCEPTION_CODES\n",
    "    - Todo código que empiece por 'TITD'\n",
    "    \"\"\"\n",
    "    if pd.isna(code):\n",
    "        return True\n",
    "    c = str(code).strip().upper()\n",
    "    return (c in EXCEPTION_CODES) or c.startswith(\"TITD\")\n",
    "\n",
    "# ------------------ Cargar datos ------------------\n",
    "# Base con asignaciones de profesores (tiene columnas: 'Nombre', 'SII - Código Materia', 'SII - Paralelo', etc.)\n",
    "profesores_df = pd.read_excel(\"data/asignaturas.xlsx\")\n",
    "\n",
    "# Limpieza mínima\n",
    "profesores_df[\"Nombre\"] = profesores_df[\"Nombre\"].astype(str).str.strip()\n",
    "\n",
    "# Renombramos la columna de código para facilidad\n",
    "profesores_df = profesores_df.rename(columns={\"SII - Código Materia\": \"codigo\"})\n",
    "# (Opcional normalización de códigos)\n",
    "# profesores_df[\"codigo\"] = profesores_df[\"codigo\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Cargar consolidado de mallas (tiene columnas: 'codigo', 'nivel', 'carrera')\n",
    "mallas_df = pd.read_csv(\"outputs/todas_mallas_codigos.csv\")\n",
    "# (Opcional normalización de códigos)\n",
    "# mallas_df[\"codigo\"] = mallas_df[\"codigo\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# ------------------ Unir mallas a profesores ------------------\n",
    "merged = profesores_df.merge(\n",
    "    mallas_df[[\"codigo\", \"nivel\", \"carrera\"]],\n",
    "    on=\"codigo\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ------------------ Mapa de múltiples paralelos ------------------\n",
    "# Para cada código, contamos cuántos paralelos distintos existen en TODA la matriz.\n",
    "# Si tiene >1 paralelo => True (múltiples paralelos).\n",
    "multi_parallel_map = (\n",
    "    profesores_df\n",
    "    .dropna(subset=[\"codigo\"])\n",
    "    .groupby(\"codigo\")[\"SII - Paralelo\"]\n",
    "    .nunique(dropna=True)\n",
    "    .gt(1)  #\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# ------------------ Construir observaciones ------------------\n",
    "observaciones = {}\n",
    "\n",
    "for nombre, grupo in merged.groupby(\"Nombre\", dropna=False):\n",
    "    problemas = []\n",
    "    # Solo filas donde conocemos nivel y carrera\n",
    "    grupo_ok = grupo.dropna(subset=[\"nivel\", \"carrera\"], how=\"any\")\n",
    "\n",
    "    for (nivel, carrera), sub in grupo_ok.groupby([\"nivel\", \"carrera\"]):\n",
    "        # Filtramos códigos no exceptuados\n",
    "        cods_filtrados = [\n",
    "            c for c in sub[\"codigo\"]\n",
    "            if pd.notna(c) and not is_exception(c)\n",
    "        ]\n",
    "        cods_unicos = sorted(set(cods_filtrados))\n",
    "\n",
    "        # Hay conflicto si existen 2+ códigos distintos en el mismo nivel/carrera\n",
    "        if len(cods_unicos) > 1:\n",
    "            # Regla: solo quitamos la observación si TODAS estas asignaturas tienen >1 paralelo\n",
    "            todas_tienen_multiples = all(multi_parallel_map.get(c, False) for c in cods_unicos)\n",
    "\n",
    "            if not todas_tienen_multiples:\n",
    "                # Al menos una tiene 1 solo paralelo -> se mantiene observación con TODOS los códigos en conflicto\n",
    "                problemas.append(\n",
    "                    f\"Malla {carrera} – Nivel {int(nivel)}: {', '.join(cods_unicos)}\"\n",
    "                )\n",
    "            # Si todas tienen >1 paralelo, NO agregamos observación (se elimina)\n",
    "\n",
    "    observaciones[nombre] = (\"• \" + \"\\n• \".join(problemas)) if problemas else \"\"\n",
    "\n",
    "# Agregar columna Observaciones al dataframe original\n",
    "profesores_df[\"Observaciones\"] = profesores_df[\"Nombre\"].map(observaciones).fillna(\"\")\n",
    "\n",
    "# Dejar la observación SOLO en la primera fila de cada profesor\n",
    "mask_dup = profesores_df.duplicated(subset=[\"Nombre\"], keep=\"first\")\n",
    "profesores_df.loc[mask_dup, \"Observaciones\"] = \"\"\n",
    "\n",
    "# ------------------ Exportar a Excel ------------------\n",
    "output_file = \"outputs/Reporte_AsignaturasPorNivel.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "    sheet_name = \"Validación\"\n",
    "    profesores_df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "    # Filtros por columna y congelar encabezado\n",
    "    n_rows, n_cols = profesores_df.shape\n",
    "    worksheet.autofilter(0, 0, n_rows, n_cols - 1)\n",
    "    worksheet.freeze_panes(1, 0)\n",
    "\n",
    "    # Formato wrap text para Observaciones\n",
    "    wrap_fmt = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "\n",
    "    # Ajuste de ancho para la columna Observaciones\n",
    "    obs_col_idx = list(profesores_df.columns).index(\"Observaciones\")\n",
    "    worksheet.set_column(obs_col_idx, obs_col_idx, 60, wrap_fmt)\n",
    "\n",
    "print(f\"✅ Archivo generado con observaciones en: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
